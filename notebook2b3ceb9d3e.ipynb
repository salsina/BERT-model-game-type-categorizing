{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:42:26.935295Z",
     "iopub.status.busy": "2022-01-22T11:42:26.934813Z",
     "iopub.status.idle": "2022-01-22T11:42:28.843412Z",
     "shell.execute_reply": "2022-01-22T11:42:28.842716Z",
     "shell.execute_reply.started": "2022-01-22T11:42:26.935200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_data = pd.read_csv(\"../input/fmnlpca2dataset/train.csv\")\n",
    "test_data = pd.read_csv(\"../input/fmnlpca2dataset/test.csv\")\n",
    "train_data = train_data[['app_id', 'description_fa', 'label']]\n",
    "test_data = test_data[['app_id', 'description_fa', 'label']]\n",
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:42:41.206420Z",
     "iopub.status.busy": "2022-01-22T11:42:41.205774Z",
     "iopub.status.idle": "2022-01-22T11:42:41.243233Z",
     "shell.execute_reply": "2022-01-22T11:42:41.242581Z",
     "shell.execute_reply.started": "2022-01-22T11:42:41.206384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>description_fa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7278</td>\n",
       "      <td>&lt;p&gt; &lt;b&gt;آموزش نحوه راه اندازی یک مرغ داری شامل ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24271</td>\n",
       "      <td>&lt;p&gt;دلتون می خواد با شخصیت های معروف دنیا سلفی ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37332</td>\n",
       "      <td>&lt;p&gt;بازی \"شکارچی\" یک بازی سرگرم کننده و مبتنی ب...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12139</td>\n",
       "      <td>&lt;p&gt;به راحتی اسم خود و عزیزانتان را به صورت کام...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25387</td>\n",
       "      <td>&lt;p&gt;در این بازی با شخصیت گارفیلد به ماجراجویی خ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30309</th>\n",
       "      <td>17517</td>\n",
       "      <td>&lt;p&gt;تاحالا ماهی گیری کردین؟! واقعا؟!! کجا؟َ! تو...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30310</th>\n",
       "      <td>25380</td>\n",
       "      <td>&lt;p&gt;«&lt;b&gt;همگو&lt;/b&gt;»؛ یک شبکه‌ی اجتماعی فارسی زبان...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30311</th>\n",
       "      <td>24025</td>\n",
       "      <td>&lt;p&gt;بازی مرد عنکبوتی  یکی از بازی های سبک هیجان...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30312</th>\n",
       "      <td>25921</td>\n",
       "      <td>&lt;p&gt;توضیح&lt;/p&gt;\\n&lt;p&gt; &lt;/p&gt;\\n&lt;p&gt; &lt;/p&gt;\\n&lt;p&gt;یک برنامه...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30313</th>\n",
       "      <td>12754</td>\n",
       "      <td>&lt;p&gt;این برنامه با قابلیت اضافه کردن به علاقه من...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30235 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       app_id                                     description_fa label\n",
       "0        7278  <p> <b>آموزش نحوه راه اندازی یک مرغ داری شامل ...     8\n",
       "1       24271  <p>دلتون می خواد با شخصیت های معروف دنیا سلفی ...     7\n",
       "2       37332  <p>بازی \"شکارچی\" یک بازی سرگرم کننده و مبتنی ب...     6\n",
       "3       12139  <p>به راحتی اسم خود و عزیزانتان را به صورت کام...     7\n",
       "4       25387  <p>در این بازی با شخصیت گارفیلد به ماجراجویی خ...     4\n",
       "...       ...                                                ...   ...\n",
       "30309   17517  <p>تاحالا ماهی گیری کردین؟! واقعا؟!! کجا؟َ! تو...     6\n",
       "30310   25380  <p>«<b>همگو</b>»؛ یک شبکه‌ی اجتماعی فارسی زبان...     9\n",
       "30311   24025  <p>بازی مرد عنکبوتی  یکی از بازی های سبک هیجان...     4\n",
       "30312   25921  <p>توضیح</p>\\n<p> </p>\\n<p> </p>\\n<p>یک برنامه...     9\n",
       "30313   12754  <p>این برنامه با قابلیت اضافه کردن به علاقه من...     8\n",
       "\n",
       "[30235 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data[train_data['label'].astype(str).str.isnumeric() == True]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:42:46.644447Z",
     "iopub.status.busy": "2022-01-22T11:42:46.644195Z",
     "iopub.status.idle": "2022-01-22T11:42:46.656509Z",
     "shell.execute_reply": "2022-01-22T11:42:46.655723Z",
     "shell.execute_reply.started": "2022-01-22T11:42:46.644417Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_train = [int(label) for label in train_data['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wordpiece_tokenizer**: tokenized into words.\n",
    "\n",
    "**subwordBased_tokenizer**: Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\". \n",
    "\n",
    "**XLNET_tokenizer**: The XLNetTokenizer uses SentencePiece for example, which is also why in the example earlier the \"▁\" character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be concatenated and \"▁\" is replaced by a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:42:50.625853Z",
     "iopub.status.busy": "2022-01-22T11:42:50.624971Z",
     "iopub.status.idle": "2022-01-22T11:43:00.249143Z",
     "shell.execute_reply": "2022-01-22T11:43:00.248418Z",
     "shell.execute_reply.started": "2022-01-22T11:42:50.625811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a8d1e91c154de698f255af1c74a8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/292 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767f35284ee54d45a01568cbc4e62b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/565 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65eaa90cb54d44a3a78e97a6ce927748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc424c6d27064192bcc542ea93b68bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c303d68ecc458482b2da5608b0d347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc36055945ec418082fc3faac3b179a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/779k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2dae6268f074dbdb7a25157d0f056b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513e8e4dfcbe41b1b084746bc1de6026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel, XLNetTokenizer\n",
    "wordpiece_tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-zwnj-base\")\n",
    "XLNET_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:43:09.380963Z",
     "iopub.status.busy": "2022-01-22T11:43:09.380572Z",
     "iopub.status.idle": "2022-01-22T11:43:27.615517Z",
     "shell.execute_reply": "2022-01-22T11:43:27.614722Z",
     "shell.execute_reply.started": "2022-01-22T11:43:09.380917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hazm\n",
      "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
      "     |████████████████████████████████| 316 kB 4.3 MB/s            \n",
      "\u001b[?25hCollecting libwapiti>=0.2.1\n",
      "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
      "     |████████████████████████████████| 233 kB 50.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk==3.3\n",
      "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
      "     |████████████████████████████████| 1.4 MB 60.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk==3.3->hazm) (1.16.0)\n",
      "Building wheels for collected packages: nltk, libwapiti\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394485 sha256=1670af4c8b04faaaabc2e1ee663f6ae58e18e5f069b3093b89fdef8d2c964929\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
      "  Building wheel for libwapiti (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=192823 sha256=8a6b3606bbffb100310536a3606e3abfab22fb9054b9aeba176539697599040f\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
      "Successfully built nltk libwapiti\n",
      "Installing collected packages: nltk, libwapiti, hazm\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.3 which is incompatible.\u001b[0m\n",
      "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Here, we create 2 tokenized datasets: first is uses word piece tokenizer, and the second one uses XLNET tokenizer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:43:27.618704Z",
     "iopub.status.busy": "2022-01-22T11:43:27.618338Z",
     "iopub.status.idle": "2022-01-22T11:44:50.634355Z",
     "shell.execute_reply": "2022-01-22T11:44:50.633586Z",
     "shell.execute_reply.started": "2022-01-22T11:43:27.618649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from hazm import *\n",
    "descreptions = train_data['description_fa']\n",
    "all_embedded_data_wordpiece = []\n",
    "all_embedded_data_XLNET = []\n",
    "s1 = []\n",
    "s2 = []\n",
    "for desc in descreptions:\n",
    "\n",
    "    desc = re.sub(r'[@|#][A-Za-z-0-9_]*|(https:[A-Za-z-0-9_:/.]*)|<p>|</p>|<b>|</br>|</b>|<br>', '', desc)\n",
    "\n",
    "    tokenized_to_word_wordpiece = wordpiece_tokenizer.encode(desc, add_special_tokens=True,)\n",
    "    all_embedded_data_wordpiece.append(tokenized_to_word_wordpiece)\n",
    "    s1.append(len(tokenized_to_word_wordpiece))\n",
    "    \n",
    "    tokenized_to_word_XLNET = XLNET_tokenizer.encode(desc, add_special_tokens=True,)\n",
    "    all_embedded_data_XLNET.append(tokenized_to_word_XLNET)\n",
    "    s2.append(len(tokenized_to_word_XLNET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's see what the average text sizes are in both tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:44:50.636425Z",
     "iopub.status.busy": "2022-01-22T11:44:50.636148Z",
     "iopub.status.idle": "2022-01-22T11:44:50.644056Z",
     "shell.execute_reply": "2022-01-22T11:44:50.643336Z",
     "shell.execute_reply.started": "2022-01-22T11:44:50.636389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average text size with word piece tokenizer:  178.12353233008102\n",
      "average text size with XLNET tokenizer:  399.2922771622292\n"
     ]
    }
   ],
   "source": [
    "avg_text_size = sum(s1)/len(s1)\n",
    "print(\"average text size with word piece tokenizer: \", avg_text_size)\n",
    "avg_text_size = sum(s2)/len(s2)\n",
    "print(\"average text size with XLNET tokenizer: \", avg_text_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are much less tokenized words in the word piece tokenizer.Moreover, the average text len is less than 512 which is a good sign to use the bert model with nothing to be worried about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding paddings and creating attention masks\n",
    "* after running the cell bellow, all texts have the same size of 512\n",
    "\n",
    "* pay attention that we do this for both of our tokenizers in order to compare their results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:44:50.645823Z",
     "iopub.status.busy": "2022-01-22T11:44:50.645386Z",
     "iopub.status.idle": "2022-01-22T11:44:57.429239Z",
     "shell.execute_reply": "2022-01-22T11:44:57.428474Z",
     "shell.execute_reply.started": "2022-01-22T11:44:50.645787Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_padding_attentionMask(all_embedded_data):\n",
    "    all_attention_mask = []\n",
    "    for i in range(len(all_embedded_data)):\n",
    "        attention_mask_text = []\n",
    "        if len(all_embedded_data[i]) < 512:\n",
    "            all_embedded_data[i] = all_embedded_data[i] + [0] * (512-len(all_embedded_data[i]))\n",
    "        else:\n",
    "            all_embedded_data[i] = all_embedded_data[i][:511] + [102]\n",
    "\n",
    "        for num in range(len(all_embedded_data[i])):\n",
    "            if all_embedded_data[i][num] > 0:\n",
    "                attention_mask_text.append(1)\n",
    "            else:\n",
    "                attention_mask_text.append(0)  \n",
    "        all_attention_mask.append(attention_mask_text) \n",
    "    \n",
    "    return all_embedded_data, all_attention_mask\n",
    "all_embedded_data_wordpiece, attentionMask_wordpiece = add_padding_attentionMask(all_embedded_data_wordpiece) \n",
    "all_embedded_data_XLNET, attentionMask_XLNET = add_padding_attentionMask(all_embedded_data_XLNET) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing input for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:44:57.431347Z",
     "iopub.status.busy": "2022-01-22T11:44:57.431100Z",
     "iopub.status.idle": "2022-01-22T11:44:57.506692Z",
     "shell.execute_reply": "2022-01-22T11:44:57.505991Z",
     "shell.execute_reply.started": "2022-01-22T11:44:57.431314Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset,DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import torch\n",
    "\n",
    "def preparing_input_for_model(all_embedded_data, all_attention_mask, batch_size):\n",
    "    #spliting dataset\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(all_embedded_data, labels_train, random_state = 2018, test_size=0.1)\n",
    "    train_mask, validation_mask, _, _ = train_test_split(all_attention_mask, labels_train, random_state = 2018, test_size=0.1)\n",
    "    \n",
    "    #changing input format to tensor\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    validation_inputs = torch.tensor(validation_inputs)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    validation_labels = torch.tensor(validation_labels)\n",
    "    train_mask = torch.tensor(train_mask)\n",
    "    validation_mask = torch.tensor(validation_mask)\n",
    "\n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order.\n",
    "    train_dataset = TensorDataset(train_inputs, train_mask, train_labels)\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                batch_size = batch_size # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataset = TensorDataset(validation_inputs, validation_mask, validation_labels)\n",
    "    validation_dataloader = DataLoader(\n",
    "                validation_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )\n",
    "    \n",
    "    return train_dataloader, validation_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Two functions which are called in our training process</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:44:57.508270Z",
     "iopub.status.busy": "2022-01-22T11:44:57.508023Z",
     "iopub.status.idle": "2022-01-22T11:44:57.515952Z",
     "shell.execute_reply": "2022-01-22T11:44:57.514028Z",
     "shell.execute_reply.started": "2022-01-22T11:44:57.508233Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>This is the function which is going to be called several times to train different models</h4>\n",
    "\n",
    "<h4>Model is created and trained in several batches and epochs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:44:57.518622Z",
     "iopub.status.busy": "2022-01-22T11:44:57.517448Z",
     "iopub.status.idle": "2022-01-22T11:44:57.544453Z",
     "shell.execute_reply": "2022-01-22T11:44:57.543732Z",
     "shell.execute_reply.started": "2022-01-22T11:44:57.518593Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def train_the_model(all_embedded_data=all_embedded_data_wordpiece, all_attention_mask=attentionMask_wordpiece, batch_size=16, model_name=\"HooshvareLab/bert-fa-zwnj-base\", learning_rate=2e-5, epochs=1):\n",
    "    train_dataloader, validation_dataloader = preparing_input_for_model(all_embedded_data, all_attention_mask, batch_size)\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "                model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "                num_labels = 10, # The number of output labels--2 for binary classification.  \n",
    "                output_attentions = False, # Whether the model returns attentions weights.\n",
    "                output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "            )\n",
    "    \n",
    "    model.cuda()\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "    \n",
    "    # Create the learning rate scheduler.\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,num_training_steps = total_steps)\n",
    "    \n",
    "    \n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Put the model into training mode.\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a backward pass. \n",
    "            model.zero_grad()        \n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # it returns the loss (because we provided labels) and \n",
    "            # the \"logits\"--the model outputs prior to activation.\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            loss = outputs[0]\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differentl during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "            # Accumulate the validation loss.\n",
    "            logits = outputs[1]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / nb_eval_steps\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "    \n",
    "    pd.set_option('precision', 2)\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "    return model, df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train our model with different hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) base version\n",
    "\n",
    "data: word piece\n",
    "\n",
    "batch size: 16\n",
    "\n",
    "BERT model: parseBert\n",
    "\n",
    "learning rate: 2e-5\n",
    "\n",
    "epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T11:57:31.776902Z",
     "iopub.status.busy": "2022-01-21T11:57:31.776627Z",
     "iopub.status.idle": "2022-01-21T12:23:13.20338Z",
     "shell.execute_reply": "2022-01-21T12:23:13.202603Z",
     "shell.execute_reply.started": "2022-01-21T11:57:31.776872Z"
    }
   },
   "outputs": [],
   "source": [
    "train_the_model(all_embedded_data_wordpiece, attentionMask_wordpiece, 16, \"HooshvareLab/bert-fa-zwnj-base\", 2e-5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Now let's change the tokenizer from word piece to XLNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: XLNET\n",
    "\n",
    "batch size: 16\n",
    "\n",
    "BERT model: parseBert\n",
    "\n",
    "learning rate: 2e-5\n",
    "\n",
    "epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T12:25:31.461357Z",
     "iopub.status.busy": "2022-01-21T12:25:31.461073Z",
     "iopub.status.idle": "2022-01-21T12:50:49.481578Z",
     "shell.execute_reply": "2022-01-21T12:50:49.480791Z",
     "shell.execute_reply.started": "2022-01-21T12:25:31.461326Z"
    }
   },
   "outputs": [],
   "source": [
    "train_the_model(all_embedded_data_XLNET, attentionMask_XLNET, 16, \"HooshvareLab/bert-fa-zwnj-base\", 2e-5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> As we can see, the loss of word piece tokenizer is less than the XLNET tokenizer; therefore, we choose word piece tokenizer </h4>\n",
    "\n",
    "# 3) Now it is time to change the bert model from parseBert into multilingualBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: **word piece**\n",
    "\n",
    "batch size: 16\n",
    "\n",
    "BERT model: multilingual\n",
    "\n",
    "learning rate: 2e-5\n",
    "\n",
    "epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T13:51:46.238895Z",
     "iopub.status.busy": "2022-01-21T13:51:46.238037Z",
     "iopub.status.idle": "2022-01-21T14:17:50.773876Z",
     "shell.execute_reply": "2022-01-21T14:17:50.773101Z",
     "shell.execute_reply.started": "2022-01-21T13:51:46.238857Z"
    }
   },
   "outputs": [],
   "source": [
    "train_the_model(all_embedded_data_wordpiece, attentionMask_wordpiece, 16, \"bert-base-multilingual-cased\", 2e-5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> As we can see, the loss of parseBert model is less than the multilingualBert model; therefore, we choose the parseBert model </h4>\n",
    "\n",
    "# 4) Now it is time to change the learning rate from 2e-5 to 2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: **word piece**\n",
    "\n",
    "batch size: 16\n",
    "\n",
    "BERT model: **parseBert**\n",
    "\n",
    "learning rate: 2e-4\n",
    "\n",
    "epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T19:49:47.271807Z",
     "iopub.status.busy": "2022-01-21T19:49:47.271591Z",
     "iopub.status.idle": "2022-01-21T20:15:24.037413Z",
     "shell.execute_reply": "2022-01-21T20:15:24.036714Z",
     "shell.execute_reply.started": "2022-01-21T19:49:47.271776Z"
    }
   },
   "outputs": [],
   "source": [
    "train_the_model(all_embedded_data_wordpiece, attentionMask_wordpiece, 16, \"HooshvareLab/bert-fa-zwnj-base\", 2e-4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> As we can see, loss increases </h4>\n",
    "\n",
    "# 5) Now it is time to change the learning rate from 2e-5 to 2e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: **word piece**\n",
    "\n",
    "batch size: 16\n",
    "\n",
    "BERT model: **parseBert**\n",
    "\n",
    "learning rate: 2e-6\n",
    "\n",
    "epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T20:17:28.075417Z",
     "iopub.status.busy": "2022-01-21T20:17:28.074858Z",
     "iopub.status.idle": "2022-01-21T20:42:37.704473Z",
     "shell.execute_reply": "2022-01-21T20:42:37.703742Z",
     "shell.execute_reply.started": "2022-01-21T20:17:28.07538Z"
    }
   },
   "outputs": [],
   "source": [
    "train_the_model(all_embedded_data_wordpiece, attentionMask_wordpiece, 16, \"HooshvareLab/bert-fa-zwnj-base\", 2e-6, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> As we can see, loss increases </h4>\n",
    "\n",
    "# 6) Now it is time to change the learning rate from 2e-5 to 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: **word piece**\n",
    "\n",
    "batch size: 16\n",
    "\n",
    "BERT model: **parseBert**\n",
    "\n",
    "learning rate: 5e-5\n",
    "\n",
    "epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T20:53:21.501674Z",
     "iopub.status.busy": "2022-01-21T20:53:21.501385Z",
     "iopub.status.idle": "2022-01-21T21:18:31.429332Z",
     "shell.execute_reply": "2022-01-21T21:18:31.428506Z",
     "shell.execute_reply.started": "2022-01-21T20:53:21.501644Z"
    }
   },
   "outputs": [],
   "source": [
    "train_the_model(all_embedded_data_wordpiece, attentionMask_wordpiece, 16, \"HooshvareLab/bert-fa-zwnj-base\", 5e-5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> As we can see, loss decreases </h4>\n",
    "\n",
    "# 7) Now it is time to change the batch size to 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: **word piece**\n",
    "\n",
    "batch size: 8\n",
    "\n",
    "BERT model: **parseBert**\n",
    "\n",
    "learning rate: **5e-5**\n",
    "\n",
    "epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:21:09.943462Z",
     "iopub.status.busy": "2022-01-21T21:21:09.943204Z",
     "iopub.status.idle": "2022-01-21T21:48:00.770504Z",
     "shell.execute_reply": "2022-01-21T21:48:00.769729Z",
     "shell.execute_reply.started": "2022-01-21T21:21:09.943435Z"
    }
   },
   "outputs": [],
   "source": [
    "train_the_model(all_embedded_data_wordpiece, attentionMask_wordpiece, 8, \"HooshvareLab/bert-fa-zwnj-base\", 5e-5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> As we can see, loss increases </h4>\n",
    "\n",
    "# 8) Now it is time to change the epoch to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: **word piece**\n",
    "\n",
    "batch size: **16**\n",
    "\n",
    "BERT model: **parseBert**\n",
    "\n",
    "learning rate: **5e-5**\n",
    "\n",
    "epoch: **5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T11:45:15.118432Z",
     "iopub.status.busy": "2022-01-22T11:45:15.117878Z",
     "iopub.status.idle": "2022-01-22T14:16:25.916998Z",
     "shell.execute_reply": "2022-01-22T14:16:25.915397Z",
     "shell.execute_reply.started": "2022-01-22T11:45:15.118393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f4a4675a684f72a8f79ef406e7a1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/452M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-fa-zwnj-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of  1,701.    Elapsed: 0:00:35.\n",
      "  Batch    80  of  1,701.    Elapsed: 0:01:09.\n",
      "  Batch   120  of  1,701.    Elapsed: 0:01:43.\n",
      "  Batch   160  of  1,701.    Elapsed: 0:02:17.\n",
      "  Batch   200  of  1,701.    Elapsed: 0:02:51.\n",
      "  Batch   240  of  1,701.    Elapsed: 0:03:26.\n",
      "  Batch   280  of  1,701.    Elapsed: 0:04:00.\n",
      "  Batch   320  of  1,701.    Elapsed: 0:04:34.\n",
      "  Batch   360  of  1,701.    Elapsed: 0:05:08.\n",
      "  Batch   400  of  1,701.    Elapsed: 0:05:42.\n",
      "  Batch   440  of  1,701.    Elapsed: 0:06:16.\n",
      "  Batch   480  of  1,701.    Elapsed: 0:06:50.\n",
      "  Batch   520  of  1,701.    Elapsed: 0:07:24.\n",
      "  Batch   560  of  1,701.    Elapsed: 0:07:59.\n",
      "  Batch   600  of  1,701.    Elapsed: 0:08:33.\n",
      "  Batch   640  of  1,701.    Elapsed: 0:09:07.\n",
      "  Batch   680  of  1,701.    Elapsed: 0:09:41.\n",
      "  Batch   720  of  1,701.    Elapsed: 0:10:15.\n",
      "  Batch   760  of  1,701.    Elapsed: 0:10:49.\n",
      "  Batch   800  of  1,701.    Elapsed: 0:11:23.\n",
      "  Batch   840  of  1,701.    Elapsed: 0:11:58.\n",
      "  Batch   880  of  1,701.    Elapsed: 0:12:32.\n",
      "  Batch   920  of  1,701.    Elapsed: 0:13:06.\n",
      "  Batch   960  of  1,701.    Elapsed: 0:13:40.\n",
      "  Batch 1,000  of  1,701.    Elapsed: 0:14:14.\n",
      "  Batch 1,040  of  1,701.    Elapsed: 0:14:48.\n",
      "  Batch 1,080  of  1,701.    Elapsed: 0:15:22.\n",
      "  Batch 1,120  of  1,701.    Elapsed: 0:15:56.\n",
      "  Batch 1,160  of  1,701.    Elapsed: 0:16:31.\n",
      "  Batch 1,200  of  1,701.    Elapsed: 0:17:05.\n",
      "  Batch 1,240  of  1,701.    Elapsed: 0:17:39.\n",
      "  Batch 1,280  of  1,701.    Elapsed: 0:18:13.\n",
      "  Batch 1,320  of  1,701.    Elapsed: 0:18:47.\n",
      "  Batch 1,360  of  1,701.    Elapsed: 0:19:21.\n",
      "  Batch 1,400  of  1,701.    Elapsed: 0:19:56.\n",
      "  Batch 1,440  of  1,701.    Elapsed: 0:20:30.\n",
      "  Batch 1,480  of  1,701.    Elapsed: 0:21:04.\n",
      "  Batch 1,520  of  1,701.    Elapsed: 0:21:38.\n",
      "  Batch 1,560  of  1,701.    Elapsed: 0:22:12.\n",
      "  Batch 1,600  of  1,701.    Elapsed: 0:22:46.\n",
      "  Batch 1,640  of  1,701.    Elapsed: 0:23:21.\n",
      "  Batch 1,680  of  1,701.    Elapsed: 0:23:55.\n",
      "\n",
      "  Average training loss: 0.90\n",
      "  Training epcoh took: 0:24:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.74\n",
      "  Validation Loss: 0.00\n",
      "  Validation took: 0:00:54\n",
      "\n",
      "======== Epoch 2 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of  1,701.    Elapsed: 0:00:34.\n",
      "  Batch    80  of  1,701.    Elapsed: 0:01:08.\n",
      "  Batch   120  of  1,701.    Elapsed: 0:01:43.\n",
      "  Batch   160  of  1,701.    Elapsed: 0:02:17.\n",
      "  Batch   200  of  1,701.    Elapsed: 0:02:51.\n",
      "  Batch   240  of  1,701.    Elapsed: 0:03:25.\n",
      "  Batch   280  of  1,701.    Elapsed: 0:03:59.\n",
      "  Batch   320  of  1,701.    Elapsed: 0:04:33.\n",
      "  Batch   360  of  1,701.    Elapsed: 0:05:08.\n",
      "  Batch   400  of  1,701.    Elapsed: 0:05:42.\n",
      "  Batch   440  of  1,701.    Elapsed: 0:06:16.\n",
      "  Batch   480  of  1,701.    Elapsed: 0:06:50.\n",
      "  Batch   520  of  1,701.    Elapsed: 0:07:24.\n",
      "  Batch   560  of  1,701.    Elapsed: 0:07:58.\n",
      "  Batch   600  of  1,701.    Elapsed: 0:08:33.\n",
      "  Batch   640  of  1,701.    Elapsed: 0:09:07.\n",
      "  Batch   680  of  1,701.    Elapsed: 0:09:41.\n",
      "  Batch   720  of  1,701.    Elapsed: 0:10:15.\n",
      "  Batch   760  of  1,701.    Elapsed: 0:10:49.\n",
      "  Batch   800  of  1,701.    Elapsed: 0:11:24.\n",
      "  Batch   840  of  1,701.    Elapsed: 0:11:58.\n",
      "  Batch   880  of  1,701.    Elapsed: 0:12:32.\n",
      "  Batch   920  of  1,701.    Elapsed: 0:13:06.\n",
      "  Batch   960  of  1,701.    Elapsed: 0:13:40.\n",
      "  Batch 1,000  of  1,701.    Elapsed: 0:14:14.\n",
      "  Batch 1,040  of  1,701.    Elapsed: 0:14:49.\n",
      "  Batch 1,080  of  1,701.    Elapsed: 0:15:23.\n",
      "  Batch 1,120  of  1,701.    Elapsed: 0:15:57.\n",
      "  Batch 1,160  of  1,701.    Elapsed: 0:16:31.\n",
      "  Batch 1,200  of  1,701.    Elapsed: 0:17:05.\n",
      "  Batch 1,240  of  1,701.    Elapsed: 0:17:40.\n",
      "  Batch 1,280  of  1,701.    Elapsed: 0:18:14.\n",
      "  Batch 1,320  of  1,701.    Elapsed: 0:18:48.\n",
      "  Batch 1,360  of  1,701.    Elapsed: 0:19:22.\n",
      "  Batch 1,400  of  1,701.    Elapsed: 0:19:56.\n",
      "  Batch 1,440  of  1,701.    Elapsed: 0:20:31.\n",
      "  Batch 1,480  of  1,701.    Elapsed: 0:21:05.\n",
      "  Batch 1,520  of  1,701.    Elapsed: 0:21:39.\n",
      "  Batch 1,560  of  1,701.    Elapsed: 0:22:13.\n",
      "  Batch 1,600  of  1,701.    Elapsed: 0:22:47.\n",
      "  Batch 1,640  of  1,701.    Elapsed: 0:23:21.\n",
      "  Batch 1,680  of  1,701.    Elapsed: 0:23:56.\n",
      "\n",
      "  Average training loss: 0.61\n",
      "  Training epcoh took: 0:24:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.75\n",
      "  Validation Loss: 0.00\n",
      "  Validation took: 0:00:54\n",
      "\n",
      "======== Epoch 3 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of  1,701.    Elapsed: 0:00:34.\n",
      "  Batch    80  of  1,701.    Elapsed: 0:01:08.\n",
      "  Batch   120  of  1,701.    Elapsed: 0:01:43.\n",
      "  Batch   160  of  1,701.    Elapsed: 0:02:17.\n",
      "  Batch   200  of  1,701.    Elapsed: 0:02:51.\n",
      "  Batch   240  of  1,701.    Elapsed: 0:03:25.\n",
      "  Batch   280  of  1,701.    Elapsed: 0:04:00.\n",
      "  Batch   320  of  1,701.    Elapsed: 0:04:34.\n",
      "  Batch   360  of  1,701.    Elapsed: 0:05:08.\n",
      "  Batch   400  of  1,701.    Elapsed: 0:05:42.\n",
      "  Batch   440  of  1,701.    Elapsed: 0:06:16.\n",
      "  Batch   480  of  1,701.    Elapsed: 0:06:51.\n",
      "  Batch   520  of  1,701.    Elapsed: 0:07:25.\n",
      "  Batch   560  of  1,701.    Elapsed: 0:07:59.\n",
      "  Batch   600  of  1,701.    Elapsed: 0:08:33.\n",
      "  Batch   640  of  1,701.    Elapsed: 0:09:07.\n",
      "  Batch   680  of  1,701.    Elapsed: 0:09:42.\n",
      "  Batch   720  of  1,701.    Elapsed: 0:10:16.\n",
      "  Batch   760  of  1,701.    Elapsed: 0:10:50.\n",
      "  Batch   800  of  1,701.    Elapsed: 0:11:24.\n",
      "  Batch   840  of  1,701.    Elapsed: 0:11:58.\n",
      "  Batch   880  of  1,701.    Elapsed: 0:12:33.\n",
      "  Batch   920  of  1,701.    Elapsed: 0:13:07.\n",
      "  Batch   960  of  1,701.    Elapsed: 0:13:41.\n",
      "  Batch 1,000  of  1,701.    Elapsed: 0:14:15.\n",
      "  Batch 1,040  of  1,701.    Elapsed: 0:14:49.\n",
      "  Batch 1,080  of  1,701.    Elapsed: 0:15:24.\n",
      "  Batch 1,120  of  1,701.    Elapsed: 0:15:58.\n",
      "  Batch 1,160  of  1,701.    Elapsed: 0:16:32.\n",
      "  Batch 1,200  of  1,701.    Elapsed: 0:17:06.\n",
      "  Batch 1,240  of  1,701.    Elapsed: 0:17:40.\n",
      "  Batch 1,280  of  1,701.    Elapsed: 0:18:15.\n",
      "  Batch 1,320  of  1,701.    Elapsed: 0:18:49.\n",
      "  Batch 1,360  of  1,701.    Elapsed: 0:19:23.\n",
      "  Batch 1,400  of  1,701.    Elapsed: 0:19:57.\n",
      "  Batch 1,440  of  1,701.    Elapsed: 0:20:31.\n",
      "  Batch 1,480  of  1,701.    Elapsed: 0:21:06.\n",
      "  Batch 1,520  of  1,701.    Elapsed: 0:21:40.\n",
      "  Batch 1,560  of  1,701.    Elapsed: 0:22:14.\n",
      "  Batch 1,600  of  1,701.    Elapsed: 0:22:48.\n",
      "  Batch 1,640  of  1,701.    Elapsed: 0:23:22.\n",
      "  Batch 1,680  of  1,701.    Elapsed: 0:23:57.\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epcoh took: 0:24:14\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.75\n",
      "  Validation Loss: 0.00\n",
      "  Validation took: 0:00:54\n",
      "\n",
      "======== Epoch 4 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of  1,701.    Elapsed: 0:00:34.\n",
      "  Batch    80  of  1,701.    Elapsed: 0:01:08.\n",
      "  Batch   120  of  1,701.    Elapsed: 0:01:43.\n",
      "  Batch   160  of  1,701.    Elapsed: 0:02:17.\n",
      "  Batch   200  of  1,701.    Elapsed: 0:02:51.\n",
      "  Batch   240  of  1,701.    Elapsed: 0:03:25.\n",
      "  Batch   280  of  1,701.    Elapsed: 0:03:59.\n",
      "  Batch   320  of  1,701.    Elapsed: 0:04:34.\n",
      "  Batch   360  of  1,701.    Elapsed: 0:05:08.\n",
      "  Batch   400  of  1,701.    Elapsed: 0:05:42.\n",
      "  Batch   440  of  1,701.    Elapsed: 0:06:16.\n",
      "  Batch   480  of  1,701.    Elapsed: 0:06:50.\n",
      "  Batch   520  of  1,701.    Elapsed: 0:07:25.\n",
      "  Batch   560  of  1,701.    Elapsed: 0:07:59.\n",
      "  Batch   600  of  1,701.    Elapsed: 0:08:33.\n",
      "  Batch   640  of  1,701.    Elapsed: 0:09:07.\n",
      "  Batch   680  of  1,701.    Elapsed: 0:09:41.\n",
      "  Batch   720  of  1,701.    Elapsed: 0:10:16.\n",
      "  Batch   760  of  1,701.    Elapsed: 0:10:50.\n",
      "  Batch   800  of  1,701.    Elapsed: 0:11:24.\n",
      "  Batch   840  of  1,701.    Elapsed: 0:11:58.\n",
      "  Batch   880  of  1,701.    Elapsed: 0:12:32.\n",
      "  Batch   920  of  1,701.    Elapsed: 0:13:06.\n",
      "  Batch   960  of  1,701.    Elapsed: 0:13:41.\n",
      "  Batch 1,000  of  1,701.    Elapsed: 0:14:15.\n",
      "  Batch 1,040  of  1,701.    Elapsed: 0:14:49.\n",
      "  Batch 1,080  of  1,701.    Elapsed: 0:15:23.\n",
      "  Batch 1,120  of  1,701.    Elapsed: 0:15:58.\n",
      "  Batch 1,160  of  1,701.    Elapsed: 0:16:32.\n",
      "  Batch 1,200  of  1,701.    Elapsed: 0:17:06.\n",
      "  Batch 1,240  of  1,701.    Elapsed: 0:17:40.\n",
      "  Batch 1,280  of  1,701.    Elapsed: 0:18:14.\n",
      "  Batch 1,320  of  1,701.    Elapsed: 0:18:48.\n",
      "  Batch 1,360  of  1,701.    Elapsed: 0:19:23.\n",
      "  Batch 1,400  of  1,701.    Elapsed: 0:19:57.\n",
      "  Batch 1,440  of  1,701.    Elapsed: 0:20:31.\n",
      "  Batch 1,480  of  1,701.    Elapsed: 0:21:05.\n",
      "  Batch 1,520  of  1,701.    Elapsed: 0:21:39.\n",
      "  Batch 1,560  of  1,701.    Elapsed: 0:22:14.\n",
      "  Batch 1,600  of  1,701.    Elapsed: 0:22:48.\n",
      "  Batch 1,640  of  1,701.    Elapsed: 0:23:22.\n",
      "  Batch 1,680  of  1,701.    Elapsed: 0:23:56.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epcoh took: 0:24:14\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.75\n",
      "  Validation Loss: 0.00\n",
      "  Validation took: 0:00:54\n",
      "\n",
      "======== Epoch 5 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of  1,701.    Elapsed: 0:00:34.\n",
      "  Batch    80  of  1,701.    Elapsed: 0:01:08.\n",
      "  Batch   120  of  1,701.    Elapsed: 0:01:43.\n",
      "  Batch   160  of  1,701.    Elapsed: 0:02:17.\n",
      "  Batch   200  of  1,701.    Elapsed: 0:02:51.\n",
      "  Batch   240  of  1,701.    Elapsed: 0:03:25.\n",
      "  Batch   280  of  1,701.    Elapsed: 0:03:59.\n",
      "  Batch   320  of  1,701.    Elapsed: 0:04:34.\n",
      "  Batch   360  of  1,701.    Elapsed: 0:05:08.\n",
      "  Batch   400  of  1,701.    Elapsed: 0:05:42.\n",
      "  Batch   440  of  1,701.    Elapsed: 0:06:16.\n",
      "  Batch   480  of  1,701.    Elapsed: 0:06:50.\n",
      "  Batch   520  of  1,701.    Elapsed: 0:07:25.\n",
      "  Batch   560  of  1,701.    Elapsed: 0:07:59.\n",
      "  Batch   600  of  1,701.    Elapsed: 0:08:33.\n",
      "  Batch   640  of  1,701.    Elapsed: 0:09:07.\n",
      "  Batch   680  of  1,701.    Elapsed: 0:09:42.\n",
      "  Batch   720  of  1,701.    Elapsed: 0:10:16.\n",
      "  Batch   760  of  1,701.    Elapsed: 0:10:50.\n",
      "  Batch   800  of  1,701.    Elapsed: 0:11:24.\n",
      "  Batch   840  of  1,701.    Elapsed: 0:11:58.\n",
      "  Batch   880  of  1,701.    Elapsed: 0:12:32.\n",
      "  Batch   920  of  1,701.    Elapsed: 0:13:07.\n",
      "  Batch   960  of  1,701.    Elapsed: 0:13:41.\n",
      "  Batch 1,000  of  1,701.    Elapsed: 0:14:15.\n",
      "  Batch 1,040  of  1,701.    Elapsed: 0:14:49.\n",
      "  Batch 1,080  of  1,701.    Elapsed: 0:15:23.\n",
      "  Batch 1,120  of  1,701.    Elapsed: 0:15:58.\n",
      "  Batch 1,160  of  1,701.    Elapsed: 0:16:32.\n",
      "  Batch 1,200  of  1,701.    Elapsed: 0:17:06.\n",
      "  Batch 1,240  of  1,701.    Elapsed: 0:17:40.\n",
      "  Batch 1,280  of  1,701.    Elapsed: 0:18:14.\n",
      "  Batch 1,320  of  1,701.    Elapsed: 0:18:49.\n",
      "  Batch 1,360  of  1,701.    Elapsed: 0:19:23.\n",
      "  Batch 1,400  of  1,701.    Elapsed: 0:19:57.\n",
      "  Batch 1,440  of  1,701.    Elapsed: 0:20:31.\n",
      "  Batch 1,480  of  1,701.    Elapsed: 0:21:05.\n",
      "  Batch 1,520  of  1,701.    Elapsed: 0:21:40.\n",
      "  Batch 1,560  of  1,701.    Elapsed: 0:22:14.\n",
      "  Batch 1,600  of  1,701.    Elapsed: 0:22:48.\n",
      "  Batch 1,640  of  1,701.    Elapsed: 0:23:22.\n",
      "  Batch 1,680  of  1,701.    Elapsed: 0:23:56.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epcoh took: 0:24:14\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.74\n",
      "  Validation Loss: 0.00\n",
      "  Validation took: 0:00:54\n",
      "\n",
      "======== Epoch 6 / 6 ========\n",
      "Training...\n",
      "  Batch    40  of  1,701.    Elapsed: 0:00:34.\n",
      "  Batch    80  of  1,701.    Elapsed: 0:01:08.\n",
      "  Batch   120  of  1,701.    Elapsed: 0:01:42.\n",
      "  Batch   160  of  1,701.    Elapsed: 0:02:17.\n",
      "  Batch   200  of  1,701.    Elapsed: 0:02:51.\n",
      "  Batch   240  of  1,701.    Elapsed: 0:03:25.\n",
      "  Batch   280  of  1,701.    Elapsed: 0:03:59.\n",
      "  Batch   320  of  1,701.    Elapsed: 0:04:33.\n",
      "  Batch   360  of  1,701.    Elapsed: 0:05:08.\n",
      "  Batch   400  of  1,701.    Elapsed: 0:05:42.\n",
      "  Batch   440  of  1,701.    Elapsed: 0:06:16.\n",
      "  Batch   480  of  1,701.    Elapsed: 0:06:50.\n",
      "  Batch   520  of  1,701.    Elapsed: 0:07:24.\n",
      "  Batch   560  of  1,701.    Elapsed: 0:07:58.\n",
      "  Batch   600  of  1,701.    Elapsed: 0:08:33.\n",
      "  Batch   640  of  1,701.    Elapsed: 0:09:07.\n",
      "  Batch   680  of  1,701.    Elapsed: 0:09:41.\n",
      "  Batch   720  of  1,701.    Elapsed: 0:10:15.\n",
      "  Batch   760  of  1,701.    Elapsed: 0:10:49.\n",
      "  Batch   800  of  1,701.    Elapsed: 0:11:24.\n",
      "  Batch   840  of  1,701.    Elapsed: 0:11:58.\n",
      "  Batch   880  of  1,701.    Elapsed: 0:12:32.\n",
      "  Batch   920  of  1,701.    Elapsed: 0:13:06.\n",
      "  Batch   960  of  1,701.    Elapsed: 0:13:40.\n",
      "  Batch 1,000  of  1,701.    Elapsed: 0:14:14.\n",
      "  Batch 1,040  of  1,701.    Elapsed: 0:14:49.\n",
      "  Batch 1,080  of  1,701.    Elapsed: 0:15:23.\n",
      "  Batch 1,120  of  1,701.    Elapsed: 0:15:57.\n",
      "  Batch 1,160  of  1,701.    Elapsed: 0:16:31.\n",
      "  Batch 1,200  of  1,701.    Elapsed: 0:17:05.\n",
      "  Batch 1,240  of  1,701.    Elapsed: 0:17:40.\n",
      "  Batch 1,280  of  1,701.    Elapsed: 0:18:14.\n",
      "  Batch 1,320  of  1,701.    Elapsed: 0:18:48.\n",
      "  Batch 1,360  of  1,701.    Elapsed: 0:19:22.\n",
      "  Batch 1,400  of  1,701.    Elapsed: 0:19:56.\n",
      "  Batch 1,440  of  1,701.    Elapsed: 0:20:31.\n",
      "  Batch 1,480  of  1,701.    Elapsed: 0:21:05.\n",
      "  Batch 1,520  of  1,701.    Elapsed: 0:21:39.\n",
      "  Batch 1,560  of  1,701.    Elapsed: 0:22:13.\n",
      "  Batch 1,600  of  1,701.    Elapsed: 0:22:47.\n",
      "  Batch 1,640  of  1,701.    Elapsed: 0:23:21.\n",
      "  Batch 1,680  of  1,701.    Elapsed: 0:23:56.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epcoh took: 0:24:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.74\n",
      "  Validation Loss: 0.00\n",
      "  Validation took: 0:00:54\n",
      "\n",
      "Training complete!\n",
      "Total training took 2:30:45 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "final_model, df_stats = train_the_model(all_embedded_data_wordpiece, attentionMask_wordpiece, 16, \"HooshvareLab/bert-fa-zwnj-base\", 5e-5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T14:16:32.265545Z",
     "iopub.status.busy": "2022-01-22T14:16:32.265115Z",
     "iopub.status.idle": "2022-01-22T14:16:32.285948Z",
     "shell.execute_reply": "2022-01-22T14:16:32.284987Z",
     "shell.execute_reply.started": "2022-01-22T14:16:32.265508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0:24:12</td>\n",
       "      <td>0:00:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0:24:13</td>\n",
       "      <td>0:00:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0:24:14</td>\n",
       "      <td>0:00:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0:24:14</td>\n",
       "      <td>0:00:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0:24:14</td>\n",
       "      <td>0:00:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0:24:13</td>\n",
       "      <td>0:00:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.90          0.0           0.74       0:24:12         0:00:54\n",
       "2               0.61          0.0           0.75       0:24:13         0:00:54\n",
       "3               0.41          0.0           0.75       0:24:14         0:00:54\n",
       "4               0.24          0.0           0.75       0:24:14         0:00:54\n",
       "5               0.14          0.0           0.74       0:24:14         0:00:54\n",
       "6               0.07          0.0           0.74       0:24:13         0:00:54"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-22T14:17:00.581400Z",
     "iopub.status.busy": "2022-01-22T14:17:00.580882Z",
     "iopub.status.idle": "2022-01-22T14:17:00.592357Z",
     "shell.execute_reply": "2022-01-22T14:17:00.591231Z",
     "shell.execute_reply.started": "2022-01-22T14:17:00.581363Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T13:36:35.524636Z",
     "iopub.status.busy": "2022-01-12T13:36:35.5243Z",
     "iopub.status.idle": "2022-01-12T13:36:53.36153Z",
     "shell.execute_reply": "2022-01-12T13:36:53.360768Z",
     "shell.execute_reply.started": "2022-01-12T13:36:35.5246Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = test_data\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "descriptions = df.description_fa.values\n",
    "test_data = test_data[test_data['label'].astype(str).str.isnumeric() == True]\n",
    "labels = test_data['label']\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "labels_test = [int(label) for label in labels]\n",
    "\n",
    "input_ids = []\n",
    "# attention_masks = []\n",
    "# # For every sentence...\n",
    "for desc in descriptions:\n",
    "    desc = re.sub(r'[@|#][A-Za-z-0-9_]*|(https:[A-Za-z-0-9_:/.]*)|<p>|</p>|<b>|</br>|</b>|<br>', '', sent)\n",
    "    encoded_desc = tokenizer.encode(desc, add_special_tokens=True,)\n",
    "    input_ids.append(encoded_desc)\n",
    "\n",
    "    \n",
    "all_attention_mask = []\n",
    "for i in range(len(input_ids)):\n",
    "    attention_mask_text = []\n",
    "    if len(input_ids[i]) < 512:\n",
    "        input_ids[i] = input_ids[i] + [0] * (512-len(input_ids[i]))\n",
    "    else:\n",
    "        input_ids[i] = input_ids[i][:511] + [102]\n",
    "    \n",
    "    for num in range(len(input_ids[i])):\n",
    "        if input_ids[i][num] > 0:\n",
    "            attention_mask_text.append(1)\n",
    "        else:\n",
    "            attention_mask_text.append(0)  \n",
    "    all_attention_mask.append(attention_mask_text) \n",
    "    \n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(all_attention_mask)\n",
    "prediction_labels = torch.tensor(labels_test)\n",
    "print(len(prediction_inputs), len(prediction_masks), len(prediction_labels))\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(test_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "  # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    print(len(outputs))\n",
    "  # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T13:49:46.636279Z",
     "iopub.status.busy": "2022-01-12T13:49:46.635589Z",
     "iopub.status.idle": "2022-01-12T13:49:47.741702Z",
     "shell.execute_reply": "2022-01-12T13:49:47.740918Z",
     "shell.execute_reply.started": "2022-01-12T13:49:46.636229Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    \n",
    "\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T13:46:43.098348Z",
     "iopub.status.busy": "2022-01-12T13:46:43.098094Z",
     "iopub.status.idle": "2022-01-12T13:46:43.108752Z",
     "shell.execute_reply": "2022-01-12T13:46:43.108011Z",
     "shell.execute_reply.started": "2022-01-12T13:46:43.098319Z"
    }
   },
   "outputs": [],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-12T13:46:16.653459Z",
     "iopub.status.busy": "2022-01-12T13:46:16.653214Z",
     "iopub.status.idle": "2022-01-12T13:46:16.674124Z",
     "shell.execute_reply": "2022-01-12T13:46:16.673407Z",
     "shell.execute_reply.started": "2022-01-12T13:46:16.653429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('Total MCC: %.3f' % mcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
